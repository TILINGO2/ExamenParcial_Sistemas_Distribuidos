DescripciÃ³n General
Este proyecto implementa dos sistemas distribuidos que demuestran conceptos fundamentales de computaciÃ³n paralela y distribuida vistos en clase:
Parte 1: Sistema de Procesamiento Distribuido (50 puntos)

Compara el rendimiento entre hilos (threads) y procesos (processes)
Implementa locks para evitar condiciones de carrera
Usa memoria compartida con multiprocessing.Value
Procesa 20 tareas con dificultad aleatoria


Software Necesario
Python 3.8 
pip3 20.0
Docker
Docker Compose 1.29
Git
Dependencia  pip3 install pymongo

Verificar Requisitos Previos
bash# Verificar Python
python3 --version
# Salida esperada: Python 3.8.0 o superior

# Verificar pip
pip3 --version
# Salida esperada: pip 20.0 o superior

# Verificar Docker
docker --version
# Salida esperada: Docker version 20.0 o superior

# Verificar Docker Compose
docker-compose --version
# Salida esperada: docker-compose version 1.29 o superior

Sofware Faltante Ubuntu
pip3 install pymongo


Instrucciones de EjecuciÃ³n
Procesamiento Distribuido
Ejecutar el Sistema
1. Navegar al directorio
cd ~/exam-sistemas-distribuidos/part1-processes-threads

# 2. Activar entorno virtual (si lo usas)
source ../venv/bin/activate

# 3. Ejecutar el procesador
python3 task_processor.py

# 4. (Opcional) Guardar output en archivo
python3 task_processor.py > resultados.txt

Salida Esperada
============================================================
SISTEMA DE PROCESAMIENTO DISTRIBUIDO
ComparaciÃ³n: Procesos vs Hilos
============================================================

ğŸ“‹ Tareas generadas:
  Task 1: Dificultad 3
  Task 2: Dificultad 5
  ...

============================================================
EJECUTANDO CON HILOS (THREADS)
============================================================
[THREAD] Task 1 completed with difficulty 3, result: 2998000 | Total completed: 1
[THREAD] Task 2 completed with difficulty 5, result: 4998000 | Total completed: 2
...

âœ… Todas las tareas completadas con hilos
â±ï¸  Tiempo total: 0.65 segundos
ğŸ“Š Tareas completadas: 20

============================================================
EJECUTANDO CON PROCESOS (PROCESSES)
============================================================
[PROCESS] Task 1 completed with difficulty 3, result: 2998000 | Total completed: 1
...

âœ… Todas las tareas completadas con procesos
â±ï¸  Tiempo total: 1.42 segundos
ğŸ“Š Tareas completadas: 20

============================================================
ANÃLISIS DE RESULTADOS
============================================================
â±ï¸  Tiempo con hilos:    0.65 segundos
â±ï¸  Tiempo con procesos: 1.42 segundos
ğŸ“ˆ Diferencia:          0.77 segundos
ğŸ† Los hilos fueron 2.18x mÃ¡s rÃ¡pidos
ğŸ’¡ RazÃ³n: Menor overhead de creaciÃ³n y comunicaciÃ³n

Â¿QuÃ© Hace Este CÃ³digo?

Genera 20 tareas con dificultad aleatoria (1-5)
Ejecuta con hilos: Crea 20 threads, cada uno procesa una tarea
Ejecuta con procesos: Crea 20 procesos, cada uno procesa una tarea
Compara tiempos: Muestra quÃ© mÃ©todo fue mÃ¡s rÃ¡pido y por quÃ©
Demuestra sincronizaciÃ³n: Usa locks para evitar race conditions

PARTE 2: Almacenamiento Distribuido
Paso 1: Iniciar Contenedores MongoDB
 Navegar al directorio raÃ­z
cd ~/exam-sistemas-distribuidos

# Iniciar contenedores en segundo plano
docker-compose up -d

# Verificar que estÃ©n corriendo
docker ps
Verificar Conectividad de MongoDB
bash# Probar conexiÃ³n al nodo 1
docker exec mongodb_node1 mongosh --eval "db.adminCommand('ping')"
# Salida esperada: { ok: 1 }

# Probar conexiÃ³n al nodo 2
docker exec mongodb_node2 mongosh --eval "db.adminCommand('ping')"
# Salida esperada: { ok: 1 }

# Ejecutar el Sistema de Almacenamiento
Navegar al directorio de parte 2
cd part2-distributed-storage


# Ejecutar el sistema
python3 storage_system.py

============================================================
SISTEMA DE ALMACENAMIENTO DISTRIBUIDO
MongoDB con 2 nodos
============================================================

ğŸ”Œ Conectando a nodos MongoDB...
  âœ… Nodo 1 conectado: mongodb://localhost:27017/
  âœ… Nodo 2 conectado: mongodb://localhost:27018/

âœ… Sistema inicializado con 2 nodos

ğŸ—‘ï¸  Limpiando todos los datos...
  Nodo 1: 0 documentos eliminados
  Nodo 2: 0 documentos eliminados
âœ… Datos limpiados

ğŸ“ Generando 100 documentos de ejemplo...

ğŸ“¤ Insertando documentos en el sistema distribuido...
ğŸ“ Documento doc_0000 â†’ Nodo 1
ğŸ“ Documento doc_0001 â†’ Nodo 2
ğŸ“ Documento doc_0002 â†’ Nodo 2
...

âœ… 100 documentos insertados

============================================================
ğŸ“Š ESTADÃSTICAS DE DISTRIBUCIÃ“N
============================================================

ğŸ“¦ Total de documentos: 100
ğŸ–¥ï¸  NÃºmero de nodos: 2

------------------------------------------------------------

Nodo 1: mongodb://localhost:27017/
  ğŸ“„ Documentos: 51
  ğŸ“Š Porcentaje: 51.0%
  ğŸ“ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

Nodo 2: mongodb://localhost:27018/
  ğŸ“„ Documentos: 49
  ğŸ“Š Porcentaje: 49.0%
  ğŸ“ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

------------------------------------------------------------

âš–ï¸  Balance de distribuciÃ³n:
  âœ… Excelente (diferencia: 2.0%)
============================================================

============================================================
ğŸ” PRUEBA DE BÃšSQUEDA DISTRIBUIDA
============================================================

ğŸ” Buscando documento: doc_0000
  â†’ Buscando en nodo esperado 1...
  âœ… Encontrado en nodo 1 (como se esperaba)

  ğŸ“„ Documento encontrado:
     ID: doc_0000
     TÃ­tulo: Documento 0
     CategorÃ­a: tecnologÃ­a
     Nodo: 1

[... mÃ¡s bÃºsquedas ...]

ğŸ”Œ Conexiones cerradas


Resultados y AnÃ¡lisis
PARTE 1: ComparaciÃ³n Hilos vs Procesos
Resultados TÃ­picos
MÃ©trica,Hilos,Procesos,Ganador
Tiempo de ejecuciÃ³n,âˆ¼0.6Â seg,âˆ¼1.4Â seg,ğŸ† Hilos
Overhead de creaciÃ³n,Bajo,Alto,ğŸ† Hilos
Uso de memoria,Compartida,Separada,ğŸ† Hilos
Paralelismo real,No (GIL),SÃ­,ğŸ† Procesos
Aislamiento,No,SÃ­,ğŸ† Procesos

Â¿Por QuÃ© los Hilos Fueron MÃ¡s RÃ¡pidos?

Tareas I/O-bound: Las tareas simulan operaciones I/O con time.sleep()
Menor overhead: Crear un thread es ~10x mÃ¡s rÃ¡pido que crear un proceso
Memoria compartida: Los hilos comparten memoria, no necesitan IPC
GIL no es problema: Para I/O, el GIL se libera automÃ¡ticamente

Â¿CuÃ¡ndo Usar Cada Uno?
Usar HILOS cuando:

 Operaciones I/O: lectura/escritura archivos, network, DB
 Muchas tareas pequeÃ±as y rÃ¡pidas
 Necesitas compartir mucha informaciÃ³n
 Ejemplo: Servidor web, web scraping, cliente API

Usar PROCESOS cuando:

 CÃ¡lculos intensivos (CPU-bound)
 Necesitas paralelismo verdadero (mÃºltiples CPUs)
 Quieres aislar fallos
 Ejemplo: Machine learning, procesamiento de imÃ¡genes, criptografÃ­a

PARTE 2: DistribuciÃ³n de Datos
Resultados de DistribuciÃ³n (100 documentos)
Nodo 1: ~50 documentos (50%)
Nodo 2: ~50 documentos (50%)
Balance: Â±2% (Excelente)

Ventajas de Hash-Based Sharding:

 DistribuciÃ³n uniforme (~50/50)
 Determinista (mismo ID â†’ mismo nodo siempre)
 Escalable (fÃ¡cil agregar mÃ¡s nodos)
 BÃºsqueda optimizada (sabemos dÃ³nde buscar)


Â¿QuÃ© hace el lock?
Solo un hilo puede entrar a la vez
Los demÃ¡s esperan su turno
Garantiza operaciones atÃ³micas

Ventajas:

 Escalabilidad horizontal (agregar mÃ¡s nodos)
 Cada nodo maneja menos datos
 Mejor rendimiento

 Estrategias:

Range-based: Rangos de IDs (0-499 â†’ Nodo1, 500-999 â†’ Nodo2)
Hash-based: Hash del ID (usado en este proyecto)
Directory-based: Tabla lookup


Conclusiones
Lecciones Aprendidas

Concurrencia es Compleja

Los locks son esenciales para evitar race conditions
Cada operaciÃ³n compartida necesita sincronizaciÃ³n
El debugging concurrente es difÃ­cil


No Hay SoluciÃ³n Universal

Hilos vs procesos depende del tipo de tarea
I/O-bound â†’ hilos
CPU-bound â†’ procesos


DistribuciÃ³n de Datos

Hash-based sharding es simple y efectivo
Balance de carga es crucial
Trade-off: consistencia vs rendimiento


Infraestructura como CÃ³digo

Docker facilita reproducibilidad
docker-compose.yml define toda la infraestructura
FÃ¡cil de escalar (agregar mÃ¡s nodos)



Aplicaciones en el Mundo Real
Sistemas que Usan Hilos/Procesos

Django/Flask: Servidores web (hilos + procesos)
Celery: Task queue distribuido (procesos)
Scrapy: Web scraping (async/threads)
NumPy/Pandas: Procesamiento paralelo (procesos)

Sistemas que Usan Sharding

MongoDB: Sharding automÃ¡tico en producciÃ³n
Cassandra: Distributed NoSQL database
Redis Cluster: In-memory cache distribuido
Elasticsearch: BÃºsqueda distribuida
HDFS: Hadoop Distributed File System

Mejoras Futuras
Si tuviera mÃ¡s tiempo, implementarÃ­a:

ReplicaciÃ³n: Cada documento en mÃºltiples nodos (tolerancia a fallos)
Consistent Hashing: Minimizar redistribuciÃ³n al agregar/quitar nodos
Rebalanceo AutomÃ¡tico: Redistribuir datos si un nodo se sobrecarga
Monitoreo: Dashboards en tiempo real (Grafana + Prometheus)
API REST: Exponer funcionalidad vÃ­a HTTP
Testing: Unit tests y integration tests
CI/CD: GitHub Actions para testing automÃ¡tico

Conceptos Demostrados

 Procesos vs Hilos: ImplementaciÃ³n y comparaciÃ³n prÃ¡ctica
 Locks y SincronizaciÃ³n: PrevenciÃ³n de race conditions
 Memoria Compartida: multiprocessing.Value
 Sharding: DistribuciÃ³n hash-based
 BÃºsqueda Distribuida: Estrategia optimista + exhaustiva
 Docker: Infraestructura containerizada
 MongoDB: Base de datos NoSQL distribuida

